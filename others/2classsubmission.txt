# Load libraries
import pandas
import csv
from numpy import vectorize
from pandas import DataFrame
from pandas.io.parsers import TextFileReader
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
import numpy as np
from sklearn.neighbors.kde import KernelDensity
import random
from sklearn.svm import SVC
from sklearn.preprocessing import normalize
from scipy.sparse import dia_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import LocalOutlierFactor
import numpy as np
import pandas as pd


#read labeld data for user i


#read labeld data
dataset = pandas.read_csv("D:/challenge/partial_labels.csv")
#for all
x,segments=dataset.axes
array=dataset.values

f = open('D:/challenge/sample_submission.csv', 'r')
reader = csv.reader(f)
mylist = list(reader)
f.close()
my_new_list = open('D:/challenge/sample_submission.csv', 'w', newline='')
res = []
line=1
for testUser in range(10,40):
    #testUser=0
   # z=np.reshape(array[testUser,51:],-1)
   # z=z.astype('int')
   # Y_train=z[0:50]
   # Y_test=z

    masquerder=[]
    #read instructions
    fileName =array[testUser,0]
    instructions=pandas.read_fwf("D:/challenge/"+fileName, header=None)
    arr=np.reshape(instructions.values,(150,100))
    user_test=np.array(arr[50:,:])
    user_train=arr[:50,:]
    hack=[]

    for i in range(10):
        labeld_data = np.reshape(array[i, 1:], -1)
        labeld_data = labeld_data.astype('int')
        fileName = array[i, 0]
        instructions = pandas.read_fwf("D:/challenge/" + fileName, header=None)
        arr = np.reshape(instructions.values, (150, 100))
        for j in range(150):
            if(labeld_data[j]==1):
                hack.append(arr[j,:])


    user_train=np.array(user_train)
    hack=np.array(hack)
    all=np.concatenate((user_train,hack))

    trainCorpus=[]
    corpus=[]
    test_corpus=[]

    for i in range(len(user_test)):
        s=' '.join((user_test[i,:]))
        test_corpus.append(s)


    for i in range(len(all)):
        s=' '.join((all[i,:]))
        if(i<50):
          trainCorpus.append(s)
          corpus.append(s)
        else:
          corpus.append(s)




    if(testUser==35):
        vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=0.4)
    else:
        vectorizer=CountVectorizer(ngram_range=(2, 2),min_df=0.5)
    vectorizer.fit_transform(trainCorpus)
    train_all=vectorizer.transform(corpus)
    x_test=vectorizer.transform(test_corpus)

##################################################################
    wordsCount={}
    uniqe=set()
    unique_feature_train=np.zeros(150)
    unique_feature_test=np.zeros(100)
    for i in range(len(all)):
        for j in range(100):
          if(i<50):
           uniqe.add(all[i][j])
           unique_feature_train[i]=random.randint(0, 10)
          else:
           if (all[i][j] not in uniqe):
            unique_feature_train[i] = random.randint(10, 20)

    for i in range(len(user_test)):
        for j in range(100):
         if (user_test[i][j] not in uniqe):
          unique_feature_test[i] = unique_feature_test[i] + 1




    ##################################################################




    train_all=np.array(train_all.toarray())
    x_test=np.array(x_test.toarray())
    classes=np.concatenate((np.zeros(50),np.ones(100)))

    train_all=np.c_[ train_all, unique_feature_train ]
    x_test=np.c_[x_test,unique_feature_test]

   # for i in range(len(train_all)):
    #    print(train_all[i,:])


    clf = SVC(gamma='auto',kernel='linear')
    clf.fit(train_all,classes)
    predicts=clf.predict(x_test)
    t=predicts

    print(testUser)
    print(t)
    index = 5000
    for i in range(len(t)):
        if (t[i] == 1):
            mylist[line][1] = 1
        else:
            mylist[line][1] = 0
    line = line + 1
    res.append(['User' + str(testUser) + '_' + str(index) + ' - ' + str(index + 100), t[i]])
    index = index + 100



csv_writer = csv.writer(my_new_list)
csv_writer.writerows(mylist)
my_new_list.close()

